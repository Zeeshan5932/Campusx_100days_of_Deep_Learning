{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# **Why We Use Integers in RNN**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What ‚ÄúUsing Integer‚Äù Means in RNN\n",
        "\n",
        "In RNNs (especially for NLP), we **do not feed words directly** to the network.\n",
        "\n",
        "Instead:\n",
        "\n",
        "* Each word/token is converted into an **integer ID**\n",
        "* These integers represent positions in a vocabulary\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "\"I love deep learning\"\n",
        "```\n",
        "\n",
        "Vocabulary:\n",
        "\n",
        "```\n",
        "I ‚Üí 1\n",
        "love ‚Üí 2\n",
        "deep ‚Üí 3\n",
        "learning ‚Üí 4\n",
        "```\n",
        "\n",
        "Sequence becomes:\n",
        "\n",
        "```\n",
        "[1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "üìå These integers are **not values**, they are **indexes**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why RNN Cannot Take Words Directly\n",
        "\n",
        "Neural networks only work with **numbers**:\n",
        "\n",
        "* No strings\n",
        "* No text\n",
        "* No symbols\n",
        "\n",
        "So text must be converted into a **numerical form**.\n",
        "\n",
        "Integers are the **simplest and most efficient representation**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Why Integers Are Preferred (Instead of One-Hot)\n",
        "\n",
        "### Option 1: One-Hot Encoding (Problematic)\n",
        "\n",
        "Vocabulary size = 10,000\n",
        "\n",
        "Word = ‚Äúlove‚Äù\n",
        "\n",
        "```\n",
        "[0,0,0,1,0,0,0,0,0,...]\n",
        "```\n",
        "\n",
        "‚ùå Huge vectors\n",
        "‚ùå Memory expensive\n",
        "‚ùå Sparse\n",
        "‚ùå Slow for RNNs\n",
        "\n",
        "---\n",
        "\n",
        "### Option 2: Integer Encoding (Efficient) ‚úÖ\n",
        "\n",
        "```\n",
        "love ‚Üí 2\n",
        "```\n",
        "\n",
        "‚úî Compact\n",
        "‚úî Fast\n",
        "‚úî Memory efficient\n",
        "‚úî Perfect for Embedding layers\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Integer Encoding + Embedding Layer (Key Concept)\n",
        "\n",
        "Integers are **NOT fed directly** into the RNN.\n",
        "\n",
        "They are passed through an **Embedding layer** first.\n",
        "\n",
        "```python\n",
        "Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "```\n",
        "\n",
        "### What happens internally:\n",
        "\n",
        "```\n",
        "Integer ‚Üí Dense vector\n",
        "```\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "2 ‚Üí [0.12, -0.34, 0.87, ...]\n",
        "```\n",
        "\n",
        "üìå This vector is **learned**, not fixed.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Why Embeddings Need Integers\n",
        "\n",
        "The embedding layer works like a **lookup table**:\n",
        "\n",
        "[\n",
        "$\\text{Embedding}(i) = W[i]$\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* `i` = integer index\n",
        "* `W` = embedding matrix\n",
        "\n",
        "If inputs were not integers ‚Üí lookup would not work.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Where Integers Are Used in RNN Pipelines\n",
        "\n",
        "‚úî Text classification\n",
        "‚úî Language modeling\n",
        "‚úî Machine translation\n",
        "‚úî Speech recognition\n",
        "‚úî Chatbots\n",
        "\n",
        "All sequence models use **integer token IDs**.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Padding Uses Integers Too\n",
        "\n",
        "Sequences have different lengths:\n",
        "\n",
        "```\n",
        "[1,2,3]\n",
        "[4,5]\n",
        "```\n",
        "\n",
        "Pad them:\n",
        "\n",
        "```\n",
        "[1,2,3]\n",
        "[4,5,0]\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `0` = padding token\n",
        "* Masked during training\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Why Not Use Float Numbers Instead?\n",
        "\n",
        "Using floats directly:\n",
        "\n",
        "* Has no semantic meaning\n",
        "* Cannot represent vocabulary position\n",
        "* Breaks embedding lookup\n",
        "\n",
        "Integers = **identity**, not magnitude.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Important Clarification (Exam Trap ‚ùó)\n",
        "\n",
        "‚ùå RNN does NOT ‚Äúlearn from integers directly‚Äù\n",
        "‚úÖ RNN learns from **embeddings derived from integers**\n",
        "\n",
        "Integers are just **keys**.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. RNN vs ANN (in this context)\n",
        "\n",
        "| Aspect     | ANN                 | RNN                 |\n",
        "| ---------- | ------------------- | ------------------- |\n",
        "| Text input | Needs preprocessing | Integer sequences   |\n",
        "| Order      | Ignored             | Preserved           |\n",
        "| Encoding   | One-hot common      | Integer + embedding |\n",
        "\n",
        "\n",
        "\n",
        "## 10. Easy Intuition (Remember This)\n",
        "\n",
        "> Integers in RNN are like **dictionary page numbers**, not meanings themselves.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Final Summary\n",
        "\n",
        "* RNNs need numeric input\n",
        "* Integers represent token IDs\n",
        "* Efficient for large vocabularies\n",
        "* Enable embedding layers\n",
        "* Preserve sequence order\n",
        "* Essential for NLP tasks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6689ca9d"
      },
      "source": [
        "This cell imports the `numpy` library, commonly used for numerical operations in Python, and defines a list of strings called `docs`. These strings will be used as a corpus for text processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SX1YuKQ4UDon"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "docs = ['go pakistan',\n",
        "\t\t'pakistan pakistan',\n",
        "\t\t'hip hip hurray',\n",
        "\t\t'jeetega bhai jeetega pakistan jeetega',\n",
        "\t\t'pakistan zinda bad',\n",
        "\t\t'Amir Amir',\n",
        "\t\t'zeeshan zeeshan',\n",
        "\t\t'dhoni dhoni',\n",
        "\t\t'cm punjab ',\n",
        "\t\t'inquilab zindabad']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pOJzJ3vzUOkh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<nothing>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2695db91"
      },
      "source": [
        "This cell imports the `Tokenizer` class from Keras' `preprocessing.text` module. It then initializes a `Tokenizer` object, specifying `oov_token='<nothing>'`. The `oov_token` (out-of-vocabulary token) handles words not found in the vocabulary during tokenization by assigning them this special token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "njO0da4-Unl8"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ddc7380"
      },
      "source": [
        "This cell fits the tokenizer on the `docs` corpus. This step builds the vocabulary based on the words present in the `docs` list. It assigns a unique integer index to each unique word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrKpFbzRUucs",
        "outputId": "ffb257cb-797f-4969-c82b-29e983c0b951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<nothing>': 1,\n",
              " 'pakistan': 2,\n",
              " 'jeetega': 3,\n",
              " 'hip': 4,\n",
              " 'amir': 5,\n",
              " 'zeeshan': 6,\n",
              " 'dhoni': 7,\n",
              " 'go': 8,\n",
              " 'hurray': 9,\n",
              " 'bhai': 10,\n",
              " 'zinda': 11,\n",
              " 'bad': 12,\n",
              " 'cm': 13,\n",
              " 'punjab': 14,\n",
              " 'inquilab': 15,\n",
              " 'zindabad': 16}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a9e3b26"
      },
      "source": [
        "This cell displays the `word_index` attribute of the tokenizer. This is a dictionary mapping each word to its corresponding integer index. The `oov_token` '<nothing>' is assigned index 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNcKSYUYegGl",
        "outputId": "452f28d9-2ceb-42a2-8fb8-a9a4aa598ae7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('go', 1),\n",
              "             ('pakistan', 5),\n",
              "             ('hip', 2),\n",
              "             ('hurray', 1),\n",
              "             ('jeetega', 3),\n",
              "             ('bhai', 1),\n",
              "             ('zinda', 1),\n",
              "             ('bad', 1),\n",
              "             ('amir', 2),\n",
              "             ('zeeshan', 2),\n",
              "             ('dhoni', 2),\n",
              "             ('cm', 1),\n",
              "             ('punjab', 1),\n",
              "             ('inquilab', 1),\n",
              "             ('zindabad', 1)])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be43d5e"
      },
      "source": [
        "This cell displays the `word_counts` attribute, which is an `OrderedDict` showing the frequency of each word in the `docs` corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx3SMLV3euuU",
        "outputId": "0262da61-e6eb-472c-f057-7375dfb02590"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.document_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf169e40"
      },
      "source": [
        "This cell displays the `document_count` attribute, which indicates the total number of documents (strings) that were used to fit the tokenizer (in this case, 10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNrqCeiUU05s",
        "outputId": "9dff4500-ee7c-4eb4-a89e-48ebdcfdb8a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[8, 2],\n",
              " [2, 2],\n",
              " [4, 4, 9],\n",
              " [3, 10, 3, 2, 3],\n",
              " [2, 11, 12],\n",
              " [5, 5],\n",
              " [6, 6],\n",
              " [7, 7],\n",
              " [13, 14],\n",
              " [15, 16]]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9576512"
      },
      "source": [
        "This cell converts the text documents into sequences of integers using the `texts_to_sequences` method of the tokenizer. Each word in the documents is replaced by its corresponding integer index from the `word_index`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y15DVHzhVCEY"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b908684a"
      },
      "source": [
        "This cell imports the `pad_sequences` function from Keras' `utils` module. This function is used to ensure that all sequences in a list have the same length, which is a common requirement for neural network inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H00D6kZlVOC4"
      },
      "outputs": [],
      "source": [
        "sequences = pad_sequences(sequences,padding='pre')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12c93655"
      },
      "source": [
        "This cell applies padding to the `sequences` generated earlier. `padding='post'` means that zeros will be added to the end of shorter sequences to match the length of the longest sequence. This creates a uniform input size for a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjL4mCzTV4rT",
        "outputId": "219777ca-eb07-4f23-d904-4b2f3e29d2c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 8,  2,  0,  0,  0],\n",
              "       [ 2,  2,  0,  0,  0],\n",
              "       [ 4,  4,  9,  0,  0],\n",
              "       [ 3, 10,  3,  2,  3],\n",
              "       [ 2, 11, 12,  0,  0],\n",
              "       [ 5,  5,  0,  0,  0],\n",
              "       [ 6,  6,  0,  0,  0],\n",
              "       [ 7,  7,  0,  0,  0],\n",
              "       [13, 14,  0,  0,  0],\n",
              "       [15, 16,  0,  0,  0]], dtype=int32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc39fa2"
      },
      "source": [
        "This cell displays the padded sequences. Notice how shorter sequences now have `0`s appended to them to match the length of the longest sequence (which has 5 elements)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "f4cMDq3KDiaC"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92536eb9"
      },
      "source": [
        "This cell imports necessary components from Keras for building a neural network: `imdb` for dataset loading, `Sequential` for creating a linear stack of layers, and `Dense`, `SimpleRNN`, `Embedding`, `Flatten` for different types of neural network layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVqDOLaefsom",
        "outputId": "7a81d93a-3952-4dee-e4e6-a498d2f50c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train,y_train),(X_test,y_test) = imdb.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "841f2054"
      },
      "source": [
        "This cell loads the IMDB movie review sentiment classification dataset using `imdb.load_data()`. It splits the dataset into training (`X_train`, `y_train`) and testing (`X_test`, `y_test`) sets. `X` contains sequences of word indices, and `y` contains sentiment labels (0 for negative, 1 for positive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTk54ntQfu6W",
        "outputId": "56ed8011-43a0-4a7c-d4b5-4687725d0cb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1afe3e7"
      },
      "source": [
        "This cell displays the first movie review from the training dataset (`X_train[0]`). This output shows a sequence of integers, where each integer represents a word in the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9tGLS4ofzBu",
        "outputId": "3a0949c9-b81b-4645-94e0-5394a221b1ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "141"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1bf48d2"
      },
      "source": [
        "This cell prints the length of the third movie review in the training dataset (`X_train[2]`), indicating the number of words (or word indices) in that particular review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uoEmzhEhf1aV"
      },
      "outputs": [],
      "source": [
        "X_train = pad_sequences(X_train,padding='post',maxlen=50)\n",
        "X_test = pad_sequences(X_test,padding='post',maxlen=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afaf669"
      },
      "source": [
        "This cell pads the sequences in both the training (`X_train`) and testing (`X_test`) datasets using `pad_sequences`. `padding='post'` adds zeros to the end, and `maxlen=50` ensures all sequences are truncated or padded to a length of 50. This is crucial for consistent input size for the RNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4kNirFSgDEe",
        "outputId": "fcf1c37e-0788-48dd-cf67-5b1d8d31c21f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2071,   56,   26,  141,    6,  194, 7486,   18,    4,  226,   22,\n",
              "         21,  134,  476,   26,  480,    5,  144,   30, 5535,   18,   51,\n",
              "         36,   28,  224,   92,   25,  104,    4,  226,   65,   16,   38,\n",
              "       1334,   88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,\n",
              "         15,   16, 5345,   19,  178,   32], dtype=int32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b41db47"
      },
      "source": [
        "This cell displays the first training sequence (`X_train[0]`) after padding. You can observe that it now has a fixed length of 50, with leading zeros if the original sequence was shorter than 50, or truncated if longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN_Fi3n6gFkp",
        "outputId": "0d228317-dd1e-48a2-a284-9bc574b754e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 32)                1088      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,121\n",
            "Trainable params: 1,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(SimpleRNN(32,input_shape=(50,1),return_sequences=False))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85e5ee9d"
      },
      "source": [
        "This cell defines a simple Recurrent Neural Network (RNN) model using Keras' `Sequential` API. It consists of two layers:\n",
        "- A `SimpleRNN` layer with 32 units. The `input_shape=(50,1)` specifies that each input sequence will have 50 timesteps, and each timestep has 1 feature (a word index). `return_sequences=False` means it will output only the last hidden state.\n",
        "- A `Dense` (fully connected) layer with 1 unit and a `sigmoid` activation function, which is suitable for binary classification tasks like sentiment analysis.\n",
        "Finally, `model.summary()` prints a summary of the model's architecture, including the number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EArIjALcgPTh",
        "outputId": "2c603e18-08ba-4754-98f0-ce4a657c0ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.6929 - accuracy: 0.5061 - val_loss: 0.6955 - val_accuracy: 0.5048\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6945 - val_accuracy: 0.5021\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.6927 - accuracy: 0.5085 - val_loss: 0.6967 - val_accuracy: 0.5008\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.6927 - accuracy: 0.5090 - val_loss: 0.6942 - val_accuracy: 0.5043\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.6928 - accuracy: 0.5056 - val_loss: 0.6957 - val_accuracy: 0.5048\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8dc97f8810>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "727acda5"
      },
      "source": [
        "This cell compiles and trains the defined RNN model:\n",
        "- `model.compile()` configures the model for training, specifying `loss='binary_crossentropy'` (appropriate for binary classification), `optimizer='adam'` (a popular optimization algorithm), and `metrics=['accuracy']` to monitor performance.\n",
        "- `model.fit()` trains the model using the padded training data (`X_train`, `y_train`) for 5 epochs. It also specifies `validation_data=(X_test, y_test)` to evaluate the model's performance on the test set after each epoch, which helps in monitoring for overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9ziz5wWgTLS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
