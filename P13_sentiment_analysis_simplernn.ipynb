{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Word Embeddings (for RNN) â€” Easy Explanation**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is an Embedding? (Simple Definition)\n",
        "\n",
        "An **embedding** is a way to **convert words into meaningful numbers** so that a neural network (like an RNN) can understand **relationships between words**.\n",
        "\n",
        "ğŸ‘‰ Instead of treating words as IDs, embeddings represent **meaning**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why We Need Embeddings (Very Important)\n",
        "\n",
        "Neural networks **cannot understand words**, only numbers.\n",
        "\n",
        "### Bad way (without embeddings):\n",
        "\n",
        "* â€œcatâ€ â†’ 1\n",
        "* â€œdogâ€ â†’ 2\n",
        "* â€œappleâ€ â†’ 3\n",
        "\n",
        "âŒ This tells the model **nothing about meaning**.\n",
        "\n",
        "---\n",
        "\n",
        "### Good way (with embeddings):\n",
        "\n",
        "* â€œcatâ€ â†’ [0.21, 0.77, 0.13, â€¦]\n",
        "* â€œdogâ€ â†’ [0.20, 0.75, 0.15, â€¦]\n",
        "* â€œappleâ€ â†’ [0.92, 0.05, 0.11, â€¦]\n",
        "\n",
        "âœ… Similar words get **similar vectors**\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Real-Life Analogy (Very Easy)\n",
        "\n",
        "### Imagine a Map ğŸ—ºï¸\n",
        "\n",
        "* Cities close together â†’ similar culture\n",
        "* Cities far apart â†’ different culture\n",
        "\n",
        "Now replace:\n",
        "\n",
        "* City â†’ Word\n",
        "* Distance â†’ Meaning similarity\n",
        "\n",
        "Example:\n",
        "\n",
        "* Paris and London â†’ close\n",
        "* Paris and Tokyo â†’ far\n",
        "\n",
        "Thatâ€™s exactly how embeddings work.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why Embeddings Are Used in RNN\n",
        "\n",
        "RNNs process sequences **step by step**.\n",
        "At each step, the input must be **meaningful numbers**.\n",
        "\n",
        "Embeddings allow the RNN to:\n",
        "\n",
        "* Understand word meaning\n",
        "* Capture context\n",
        "* Learn sentence structure\n",
        "\n",
        "Example sentence:\n",
        "\n",
        "> â€œI love deep learningâ€\n",
        "\n",
        "RNN sees:\n",
        "\n",
        "* â€œIâ€ â†’ meaning vector\n",
        "* â€œloveâ€ â†’ meaning vector\n",
        "* â€œdeepâ€ â†’ meaning vector\n",
        "\n",
        "Instead of random numbers.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. What Problem Embeddings Solve\n",
        "\n",
        "### Without embeddings:\n",
        "\n",
        "> â€œkingâ€ and â€œqueenâ€ are unrelated âŒ\n",
        "\n",
        "### With embeddings:\n",
        "\n",
        "> â€œkingâ€ and â€œqueenâ€ are close in meaning âœ…\n",
        "\n",
        "So the RNN learns:\n",
        "\n",
        "* Grammar\n",
        "* Semantics\n",
        "* Relationships\n",
        "\n",
        "---\n",
        "\n",
        "## 6. What Are Pretrained Embeddings?\n",
        "\n",
        "**Pretrained embeddings** are embeddings that were **already learned** using **huge text datasets**.\n",
        "\n",
        "They already know:\n",
        "\n",
        "* Grammar\n",
        "* Word meaning\n",
        "* Context\n",
        "* Relationships\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Word2Vec\n",
        "* GloVe\n",
        "* FastText\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Why We Use Pretrained Embeddings\n",
        "\n",
        "Training embeddings from scratch needs:\n",
        "\n",
        "* Massive text data\n",
        "* Time\n",
        "* Compute\n",
        "\n",
        "Pretrained embeddings help because:\n",
        "\n",
        "âœ… They already understand language\n",
        "âœ… Work well with small datasets\n",
        "âœ… Improve accuracy\n",
        "âœ… Reduce training time\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Real-Life Example (Very Intuitive)\n",
        "\n",
        "### Learning Language\n",
        "\n",
        "#### Without pretrained embeddings:\n",
        "\n",
        "You teach a child **only from your own notes**.\n",
        "\n",
        "#### With pretrained embeddings:\n",
        "\n",
        "You give the child a **dictionary + encyclopedia** first.\n",
        "\n",
        "ğŸ‘‰ Which one learns faster and better?\n",
        "**Pretrained embeddings**.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Example in Sentiment Analysis\n",
        "\n",
        "Sentence:\n",
        "\n",
        "> â€œThis movie is fantasticâ€\n",
        "\n",
        "Because of embeddings:\n",
        "\n",
        "* â€œfantasticâ€ is close to â€œgreatâ€, â€œawesomeâ€\n",
        "* Far from â€œbadâ€, â€œterribleâ€\n",
        "\n",
        "So RNN easily predicts **positive sentiment**.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Why Pretrained Embeddings Are Powerful in RNN\n",
        "\n",
        "RNNs already struggle with:\n",
        "\n",
        "* Long sequences\n",
        "* Limited data\n",
        "\n",
        "Embeddings:\n",
        "\n",
        "* Reduce learning burden\n",
        "* Provide semantic foundation\n",
        "* Make learning stable\n",
        "\n",
        "ğŸ‘‰ They work together:\n",
        "**Embeddings = meaning**, **RNN = memory**.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. When to Use Pretrained Embeddings\n",
        "\n",
        "âœ… Small or medium dataset\n",
        "âœ… NLP tasks (sentiment, classification, QA)\n",
        "âœ… When language understanding matters\n",
        "\n",
        "---\n",
        "\n",
        "## 12. When Not Necessary\n",
        "\n",
        "âŒ Very large custom text dataset\n",
        "âŒ Domain-specific words only (medical, legal)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 13. Ultra-Simple Summary\n",
        "\n",
        "* Embeddings = word meaning in numbers\n",
        "* RNNs need embeddings to understand text\n",
        "* Pretrained embeddings already know language\n",
        "* Faster training + better accuracy\n",
        "\n",
        "---\n",
        "\n",
        "## 14. One Sentence Intuition (Best to Remember)\n",
        "\n",
        "> Embeddings teach a model what words mean, and RNN teaches it how words come together over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "z4KFSMQOUXsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vR6C9i-fyLvG"
      },
      "outputs": [],
      "source": [
        "docs = ['go pakistan',\n",
        "\t\t'pakistan pakistan',\n",
        "\t\t'hip hip hurray',\n",
        "\t\t'jeetega bhai jeetega pakistan jeetega',\n",
        "\t\t'pakistan zinda bad',\n",
        "\t\t'Amir Amir',\n",
        "\t\t'zeeshan zeeshan',\n",
        "\t\t'dhoni dhoni',\n",
        "\t\t'cm punjab ',\n",
        "\t\t'inquilab zindabad']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<nothing>')"
      ],
      "metadata": {
        "id": "xRzvJNh1mRQW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "2IhkPg6YmRYT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJnGjX0xVRos",
        "outputId": "d30fb85d-1a29-4f03-ccc9-8daaab6a7756"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<nothing>': 1,\n",
              " 'pakistan': 2,\n",
              " 'jeetega': 3,\n",
              " 'hip': 4,\n",
              " 'amir': 5,\n",
              " 'zeeshan': 6,\n",
              " 'dhoni': 7,\n",
              " 'go': 8,\n",
              " 'hurray': 9,\n",
              " 'bhai': 10,\n",
              " 'zinda': 11,\n",
              " 'bad': 12,\n",
              " 'cm': 13,\n",
              " 'punjab': 14,\n",
              " 'inquilab': 15,\n",
              " 'zindabad': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP5g_Tmin4AG",
        "outputId": "31adb1a7-bd5f-4931-8183-db5a06cc5138"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOv8NrFFmRcE",
        "outputId": "6672fb61-e4a8-4b46-d8d7-258bf67b0926"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 2],\n",
              " [2, 2],\n",
              " [4, 4, 9],\n",
              " [3, 10, 3, 2, 3],\n",
              " [2, 11, 12],\n",
              " [5, 5],\n",
              " [6, 6],\n",
              " [7, 7],\n",
              " [13, 14],\n",
              " [15, 16]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "sequences = pad_sequences(sequences,padding='post')\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG1RqcLnmRkA",
        "outputId": "89d71a3b-744f-4b02-8fdc-502cf4689551"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8,  2,  0,  0,  0],\n",
              "       [ 2,  2,  0,  0,  0],\n",
              "       [ 4,  4,  9,  0,  0],\n",
              "       [ 3, 10,  3,  2,  3],\n",
              "       [ 2, 11, 12,  0,  0],\n",
              "       [ 5,  5,  0,  0,  0],\n",
              "       [ 6,  6,  0,  0,  0],\n",
              "       [ 7,  7,  0,  0,  0],\n",
              "       [13, 14,  0,  0,  0],\n",
              "       [15, 16,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(17,output_dim=2,input_length=5))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "ETWEOn4wmRzd",
        "outputId": "fa2ac2c4-5051-46e5-a260-6f8ec1a67043"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile('adam','accuracy')"
      ],
      "metadata": {
        "id": "R68ghDfNmSAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(sequences)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooYWO2NwoKoQ",
        "outputId": "62b763d3-07b0-49d5-a2ca-23fe55c4ee7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "[[[-0.03378457 -0.0223366 ]\n",
            "  [-0.01869411  0.0397962 ]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[-0.01869411  0.0397962 ]\n",
            "  [-0.01869411  0.0397962 ]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[ 0.00716299  0.02782274]\n",
            "  [ 0.00716299  0.02782274]\n",
            "  [-0.02727993 -0.02907238]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[ 0.04634063 -0.01492285]\n",
            "  [ 0.04544855 -0.02214533]\n",
            "  [ 0.04634063 -0.01492285]\n",
            "  [-0.01869411  0.0397962 ]\n",
            "  [ 0.04634063 -0.01492285]]\n",
            "\n",
            " [[ 0.03687553  0.00542127]\n",
            "  [ 0.00882751  0.02693466]\n",
            "  [ 0.00960891 -0.02113671]\n",
            "  [-0.01385436  0.00159916]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[ 0.04272473 -0.02110641]\n",
            "  [ 0.04272473 -0.02110641]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[ 0.01033523 -0.02812191]\n",
            "  [ 0.01033523 -0.02812191]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[ 0.0243187  -0.02920236]\n",
            "  [ 0.0243187  -0.02920236]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[ 0.03549886  0.00285759]\n",
            "  [ 0.0033599  -0.03792955]\n",
            "  [ 0.00960891 -0.02113671]\n",
            "  [-0.01385436  0.00159916]\n",
            "  [ 0.04738592  0.03497881]]\n",
            "\n",
            " [[-0.03086355  0.04078293]\n",
            "  [ 0.          0.        ]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]\n",
            "  [ 0.04738592  0.03497881]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ],
      "metadata": {
        "id": "yrM0IXVamPLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = imdb.load_data()"
      ],
      "metadata": {
        "id": "4aH4HVjcyq1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train,padding='post',maxlen=50)\n",
        "X_test = pad_sequences(X_test,padding='post',maxlen=50)"
      ],
      "metadata": {
        "id": "cu2UOZGUzAEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO412Krkz7EO",
        "outputId": "b4f2de4f-c1ba-4d76-c8b9-cf7087850a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 2,50))\n",
        "model.add(SimpleRNN(32,return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWXhm8vP_DO-",
        "outputId": "c3eb5f20-a438-4439-b5fe-640f215fb61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 2)           20000     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 32)                1120      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,153\n",
            "Trainable params: 21,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train,epochs=5,validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGCAZ7Rm_fqH",
        "outputId": "6f6a73ab-fbd4-4a23-ea56-46e3f9358c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 40s 50ms/step - loss: 0.5801 - acc: 0.6731 - val_loss: 0.4433 - val_acc: 0.7978\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.3524 - acc: 0.8510 - val_loss: 0.4430 - val_acc: 0.8051\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.2040 - acc: 0.9224 - val_loss: 0.5460 - val_acc: 0.7828\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 0.0823 - acc: 0.9735 - val_loss: 0.6990 - val_acc: 0.7824\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 0.0352 - acc: 0.9894 - val_loss: 0.8774 - val_acc: 0.7670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jAQETwiZikEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}